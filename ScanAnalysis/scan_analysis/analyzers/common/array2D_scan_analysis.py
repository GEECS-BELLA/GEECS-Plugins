"""
Array2DScanAnalyzer

General analyzer for 2D array type data.
Child to ScanAnalysis (./scan_analysis/base.py)

This provides a general framework for using an ImageAnalyzer to do analysis and post processing
over scan.

It will generate some default type of visual representation of the data. For a 'noscan' it will
create an average image and a .gif. For a parameter scan, it will create an image array with each
individual image averaged over the bin # (e.g. scan parameter).

It can make use of multi threaded or multi processing of the images. The method applied is determined
by the attribute self.run_analyze_image_asynchronously in the ImageAnalyzer class. A setting of False
will use parallel processes and True will run multi threaded. It seems the multi processing seems to
be slightly more reliable. Issues with threading and saving images have been noticed.

The method used for executing the ImageAnalysis.analyze_image method is process_shot_parallel (a static method).
It is expected that the ImageAnalysis.analyze_image method will return a dict generated by the 'build_return_dict'
method of the base ImageAnalysis. This dict looks like this:
        return_dictionary = {
            "analyzer_input_parameters": input_parameters,
            "analyzer_return_dictionary": return_scalars,
            "processed_image_uint16": uint_image,
            "analyzer_return_lineouts": return_lineouts,
        }
The 'processed_image_uint16' item is the on that will be used to generate the visualized data.
The "analyzer_return_dictionary" contains a dict[str, float] that is used to write scalar
data to the 'sxxx.txt' file.
There is a rudimentary way to handle other types of return data using "analyzer_return_lineouts".
For example, a camera may be use as a spectrometer which is intended to generate a 1D array of
wavelength (energy) vs counts. Running this to through the default post_processing methods in
Array2DScanAnalysis is not appropriate. By default, Array2DScanAnalysis will ignore the lineout
data. But, child classes can be created to make use of the info by overwriting just a couple of
methods. An example is the HIMG_with_average_saving file. Creating a child class does still make
use of the other key parts of this framework (e.g. parallel processing, appending scalar data)

"""
# %% imports
from __future__ import annotations

from typing import TYPE_CHECKING, Union, Optional, List


if TYPE_CHECKING:
    from geecs_python_api.controls.api_defs import ScanTag
    from numpy.typing import NDArray

from pathlib import Path
import logging
import numpy as np
import numbers
import cv2
import matplotlib

use_interactive = False
if not use_interactive:
    matplotlib.use("Agg")

import matplotlib.pyplot as plt
from matplotlib.colors import Normalize

import imageio as io

from scan_analysis.base import ScanAnalysis
from image_analysis.base import ImageAnalyzer

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

import traceback
PRINT_TRACEBACK = True

# %% classes
class Array2DScanAnalysis(ScanAnalysis):

    def __init__(self, scan_tag: ScanTag,
                 device_name: str,
                 image_analyzer: Optional[ImageAnalyzer] = None,
                 skip_plt_show: bool = True,
                 flag_logging: bool = True,
                 flag_save_images: bool = True):
        """
        Initialize the CameraImageAnalysis class.

        Args:
            scan_tag (ScanTag): Path to the scan directory containing data.
            device_name (str): Name of the device to construct the subdirectory path.
            skip_plt_show (bool): Flag that sets if matplotlib is tried to use for plotting
            flag_logging (bool): Flag that sets if error and warning messages are displayed
            flag_save_images (bool): Flag that sets if images are saved to disk
        """
        if not device_name:
            raise ValueError("CameraImageAnalysis requires a device_name.")

        super().__init__(scan_tag, device_name=device_name,
                         skip_plt_show=skip_plt_show)

        self.image_analyzer = image_analyzer or ImageAnalyzer()
        # define flags
        self.flag_logging = flag_logging
        self.flag_save_images = flag_save_images

        self.file_pattern: str = "*_{shot_num:03d}.png"

        # organize various paths
        self.path_dict = {'data_img': Path(self.scan_directory) / f"{device_name}",
                          'save': (self.scan_directory.parents[1] / 'analysis' / self.scan_directory.name
                                   / f"{device_name}" / "Array2DScanAnalysis")
                          }

        self.data = {'shot_num': [], 'images': []}

        # Check if data directory exists and is not empty
        if not self.path_dict['data_img'].exists() or not any(self.path_dict['data_img'].iterdir()):
            if self.flag_logging:
                logging.warning(f"Data directory '{self.path_dict['data_img']}' does not exist or is empty. Skipping")

    def run_analysis(self, config_options: Optional[str] = None):
        if self.path_dict['data_img'] is None or self.auxiliary_data is None:
            if self.flag_logging:
                logging.info("Skipping analysis due to missing data or auxiliary file.")
            return

        if config_options is not None:
            raise NotImplementedError

        if self.flag_save_images and not self.path_dict['save'].exists():
            self.path_dict['save'].mkdir(parents=True)

        try:
            # # Process each shot in a common method.
            # self._process_all_shots()

            # Run the image analyzer on every shot in parallel.
            self._process_all_shots_parallel()

            # Depending on the scan type, perform additional processing.
            if len(self.data['images'])>2:
                if self.noscan:
                    self._postprocess_noscan()
                else:
                    if use_interactive:
                        self._postprocess_scan_interactive()
                    else:
                        self._postprocess_scan_parallel()

            self.auxiliary_data.to_csv(self.auxiliary_file_path, sep='\t', index=False)
            return self.display_contents

        except Exception as e:
            if PRINT_TRACEBACK:
                print(traceback.format_exc())
            if self.flag_logging:
                logging.warning(f"Warning: Image analysis failed due to: {e}")
            return

    @staticmethod
    def process_shot_parallel(
            shot_num: int, file_path: Path, image_analyzer: ImageAnalyzer
    ) -> tuple[int, Optional[np.ndarray], dict]:
        """
        Helper function for parallel processing in a separate process.
        Creates a new analyzer instance from analyzer_class, processes the image,
        and returns the shot number, processed image, and analysis results.

        If the analyzer's return value is not as expected (e.g., not a dict, missing
        keys, or values are None), it logs a warning and returns safe defaults.
        """
        try:
            analyzer = image_analyzer
            results_dict = analyzer.analyze_image_file(image_filepath=file_path)
        except Exception as e:
            logging.error(f"Error during analysis for shot {shot_num}: {e}")
            return shot_num, None, {}

        if not isinstance(results_dict, dict):
            logging.warning(f"Analyzer returned non-dict result for shot {shot_num}.")
            return shot_num, None, {}

        if "processed_image_uint16" not in results_dict:
            logging.warning(f"Shot {shot_num}: 'processed_image_uint16' key not found in analyzer result.")
            image = None
        else:
            image = results_dict.get("processed_image_uint16")

        analysis = results_dict.get("analyzer_return_dictionary", {})
        if not isinstance(analysis, dict):
            logging.warning(f"Shot {shot_num} analysis returned non-dict 'analyzer_return_dictionary'.")
            analysis = {}

        if image is None and analysis:
            logging.info(f"Shot {shot_num} returned no processed image, but analysis results are available.")
        elif image is None:
            logging.warning(f"Shot {shot_num} returned no processed image or analysis results.")

        return shot_num, image, analysis

    def _process_all_shots_parallel(self) -> None:
        """
        Run the image analyzer on every shot in parallel and update self.data.
        This method uses both a ProcessPoolExecutor and a ThreadPoolExecutor
        (depending on whether the analyzer should run asynchronously) and mirrors
        the error handling and logging from your sample code.
        """
        # Clear existing data
        self.data = {'shot_num': [], 'images': []}

        # Define success and error handlers:
        def process_success(shot_number: int, analysis_result: dict) -> None:
            """
            On successful analysis of a shot, update self.data and auxiliary_data.
            If no processed image is returned but analysis results are available,
            update the auxiliary data and log the outcome, but do not add to self.data.
            """
            image = analysis_result.get("processed_image_uint16")
            analysis = analysis_result.get("analyzer_return_dictionary", {})

            # Always update auxiliary data if analysis exists.
            if analysis:
                for key, value in analysis.items():
                    if not isinstance(value, numbers.Number):
                        logging.warning(
                            f"[{self.__class__.__name__} using {self.image_analyzer.__class__.__name__}] "
                            f"Analysis result for shot {shot_number} key '{key}' is not numeric (got {type(value).__name__}). Skipping."
                        )
                    else:
                        self.auxiliary_data.loc[self.auxiliary_data['Shotnumber'] == shot_number, key] = value
                logging.info(f"Finished processing analysis for shot {shot_number}.")
            else:
                logging.warning(f"No analysis results returned for shot {shot_number}.")

            # If a processed image is available, update self.data.
            if image is not None:
                # try:
                #     image = image.astype(np.uint16)
                # except Exception as e:
                #     logging.error(f"Error converting image for shot {shot_number} to uint16: {e}")
                #     image = None
                if image is not None:
                    self.data['shot_num'].append(shot_number)
                    self.data['images'].append(image)
            else:
                logging.info(f"Shot {shot_number} returned no processed image; only auxiliary data was updated.")

        def process_error(shot_number: int, exception: Exception) -> None:
            """
            Log an error if processing a shot fails.
            """
            logging.error(f"Error while analyzing shot {shot_number}: {exception}")

        # Gather tasks: each shot number paired with its file path.
        tasks = []
        for shot_num in self.auxiliary_data['Shotnumber'].values:
            pattern = self.file_pattern.format(shot_num=shot_num)
            file_path = next(self.path_dict['data_img'].glob(pattern), None)
            logging.info(f'file path found is {file_path}')
            if file_path is not None:
                tasks.append((shot_num, file_path))

        # Dictionary to map futures to shot numbers.
        image_analysis_futures = {}

        # Submit image analysis jobs.
        # Use ProcessPoolExecutor if the analyzer is CPU-bound,
        # or ThreadPoolExecutor if run_analyze_image_asynchronously is True.

        with ProcessPoolExecutor(max_workers=4) as process_pool, ThreadPoolExecutor(max_workers=4) as thread_pool:
            for shot_num, file_path in tasks:
                if self.image_analyzer.run_analyze_image_asynchronously:
                    # For asynchronous (likely I/O-bound) processing, use the thread pool.
                    future = thread_pool.submit(self.image_analyzer.analyze_image_file, image_filepath=file_path)
                else:
                    # For CPU-bound tasks, use the process pool.
                    # Pass the analyzer's class so each process can create its own instance.
                    future = process_pool.submit(self.process_shot_parallel, shot_num, file_path,
                                                 self.image_analyzer)
                image_analysis_futures[future] = shot_num

            # Process completed futures.
            for future in as_completed(image_analysis_futures):
                shot_num = image_analysis_futures[future]
                if (exception := future.exception()) is not None:
                    process_error(shot_num, exception)
                else:
                    result = future.result()
                    # If using the process pool, result is a tuple: (shot_num, image, analysis).
                    if isinstance(result, tuple):
                        _, image, analysis = result
                        analysis_result = {"processed_image_uint16": image, "analyzer_return_dictionary": analysis}
                    else:
                        analysis_result = result
                    process_success(shot_num, analysis_result)

    def _postprocess_noscan(self) -> None:
        """Perform post-processing for a no-scan: average images and create a GIF."""
        avg_image = self.average_images(self.data['images'])
        if self.flag_save_images:
            self.save_geecs_scaled_image(avg_image, save_dir=self.path_dict['save'],
                                         save_name=f'{self.device_name}_average_processed.png')
            save_name = f'{self.device_name}_average_processed_visual.png'
            self.save_normalized_image(avg_image, save_dir=self.path_dict['save'],
                                       save_name=save_name, label=save_name)
            display_content_path = Path(self.path_dict['save']) / save_name
            self.display_contents.append(str(display_content_path))
            # Create GIF
            filepath = self.path_dict['save'] / 'noscan.gif'
            self.create_gif(self.data['images'], filepath,
                            titles=[f"Shot {num}" for num in self.data['shot_num']])
            self.display_contents.append(str(filepath))

    def _save_bin_images(self, bin_key: int, processed_image: np.ndarray) -> None:
        """
        Helper method to save images for a single bin.
        This saves both the scaled and normalized images.
        """
        save_name_scaled = f"{self.device_name}_{bin_key}_processed.png"
        save_name_normalized = f"{self.device_name}_{bin_key}_processed_visual.png"
        self.save_geecs_scaled_image(processed_image,
                                     save_dir=self.path_dict["save"],
                                     save_name=save_name_scaled)
        self.save_normalized_image(processed_image,
                                   save_dir=self.path_dict["save"],
                                   save_name=save_name_normalized)
        if self.flag_logging:
            logging.info(f"Saved bin {bin_key} images: {save_name_scaled} and {save_name_normalized}")

    def _postprocess_scan_parallel(self) -> None:
        """
        Post-process a scanned variable by binning the images (from self.data) and then
        saving the resulting images in parallel.
        """
        # Use your existing parallel (or sequential) binning method to create binned_data.
        binned_data = self.bin_images_from_data(flag_save=False)  # we handle saving separately

        # Save each bin's images concurrently using a thread pool.
        if self.flag_save_images:
            with ThreadPoolExecutor() as executor:
                futures = []
                for bin_key, bin_item in binned_data.items():
                    processed_image = bin_item["image"]
                    futures.append(executor.submit(self._save_bin_images, bin_key, processed_image))
                for future in as_completed(futures):
                    # Optionally handle exceptions:
                    try:
                        future.result()
                    except Exception as e:
                        logging.error(f"Error saving images for a bin: {e}")

        # Create an image grid if more than one bin exists.
        if len(binned_data) > 1 and self.flag_save_images:
            plot_scale = (getattr(self, "camera_analysis_settings", {}) or {}).get("Plot Scale", None)
            save_path = Path(self.path_dict["save"]) / f'{self.device_name}_averaged_image_grid.png'
            self.create_image_array(binned_data, plot_scale=plot_scale, save_path=save_path)
            self.display_contents.append(str(save_path))
        self.binned_data = binned_data

    def _postprocess_scan_interactive(self) -> None:
        """Perform post-processing for a scan: bin images and create an image grid."""
        # Bin images from the already processed data.
        # Here we let bin_images_from_data handle the grouping without saving,
        # as we'll do the saving in our helper.
        binned_data = self.bin_images_from_data(flag_save=False)

        # Process each bin sequentially.
        for bin_key, bin_item in binned_data.items():
            processed_image = bin_item.get("image")
            if self.flag_save_images and processed_image is not None:
                self._save_bin_images(bin_key, processed_image)
            elif processed_image is None:
                logging.warning(f"Bin {bin_key} has no processed image; skipping saving for this bin.")

        # If more than one bin exists, create an image grid.
        if len(binned_data) > 1 and self.flag_save_images:
            plot_scale = (getattr(self, "camera_analysis_settings", {}) or {}).get("Plot Scale", None)
            save_path = Path(self.path_dict["save"]) / f"{self.device_name}_averaged_image_grid.png"
            self.create_image_array(binned_data, plot_scale=plot_scale, save_path=save_path)
            self.display_contents.append(str(save_path))

        self.binned_data = binned_data

    def bin_images_from_data(self, flag_save: Optional[bool] = None) -> dict:
        """
        Groups the already processed images (stored in self.data) by their bin value
        (as defined in self.auxiliary_data) and averages the images for each bin.

        Args:
            flag_save (bool): Whether to save the binned images. Defaults to self.flag_save_images.

        Returns:
            dict: A dictionary mapping each bin value to a dict with keys 'value' (the bin parameter)
                  and 'image' (the averaged image).
        """
        if flag_save is None:
            flag_save = self.flag_save_images

        # Assume that self.auxiliary_data contains a column "Bin #" and that each shot number in self.data['shot_num']
        # corresponds to an entry in self.auxiliary_data. Also, assume self.scan_parameter holds the name of the parameter
        # used for binning.
        unique_bins = np.unique(self.auxiliary_data["Bin #"].values)
        if self.flag_logging:
            logging.info(f"Unique bins from auxiliary data: {unique_bins}")

        binned_data = {}
        # Loop over each bin value
        for bin_val in unique_bins:
            # Get the shot numbers that belong to this bin.
            bin_shots = self.auxiliary_data[self.auxiliary_data["Bin #"] == bin_val]["Shotnumber"].values
            # Find the indices in self.data that match these shot numbers.
            indices = [i for i, shot in enumerate(self.data["shot_num"]) if shot in bin_shots]
            if not indices:
                if self.flag_logging:
                    logging.warning(f"No images found for bin {bin_val}.")
                continue

            # Gather the images for this bin.
            images = [self.data["images"][i] for i in indices]
            # Average the images (using your average_images method)
            avg_image = self.average_images(images)
            # Get a representative parameter value for the bin.
            # Here we assume the auxiliary data contains a column with self.scan_parameter.


            column_full_name, column_alias = self.find_scan_param_column()

            param_value = self.auxiliary_data.loc[self.auxiliary_data["Bin #"] == bin_val, column_full_name].mean()

            binned_data[bin_val] = {"value": param_value, "image": avg_image}

            if flag_save:
                save_name = f"{self.device_name}_{bin_val}.png"
                self.save_geecs_scaled_image(avg_image,
                                             save_dir=self.path_dict["save"],
                                             save_name=save_name)
                if self.flag_logging:
                    logging.info(f"Binned and averaged images for bin {bin_val} saved as {save_name}.")

        return binned_data

    def save_fig(self, save_path: Path,
                 bbox_inches: str = 'tight', pad_inches: float = 0.) -> None:

        # ensure save directory exists
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # save image
        plt.savefig(save_path, bbox_inches=bbox_inches, pad_inches=pad_inches)

    def save_geecs_scaled_image(self, image: NDArray, save_dir: Union[str, Path],
                                save_name: str, bit_depth: int = 16):
        """
        Images saved through GEECS typically are saved as 16bit, but the hardware saves
        12 bit. In other words, the last 4 bits are unused. This method will save as
        16 bit image or, if 8bit representation is desired for visualization, it will
        scale to 8 bits properly

        Args:
            image (np.ndarray): The image to save.
            save_dir (str or Path): Directory where the image will be saved.
            save_name (str): The name of the saved image file.
            bit_depth (int): The bit depth of the saved image (default is 16-bit).
        """
        save_path = Path(save_dir) / save_name

        # Ensure the directory exists
        save_path.parent.mkdir(parents=True, exist_ok=True)

        # Convert to 16-bit if required
        if bit_depth == 16:
            if image.dtype == np.uint8:
                image = (image.astype(np.uint16)) * 256  # Scale 8-bit to 16-bit
            elif image.dtype != np.uint16:
                raise ValueError("Image must be either 8-bit or 16-bit format.")
        elif bit_depth == 8:
            image = (image * 2 ** 4).astype(np.uint8)
        else:
            raise ValueError("Unsupported bit depth. Only 8 or 16 bits are supported.")

        # Save the image using cv2
        cv2.imwrite(str(save_path), image)
        if self.flag_logging:
            logging.info(f"Image saved at {save_path}")

    def save_normalized_image(self, image: np.ndarray, save_dir: Union[str, Path], save_name: str,
                              label: Optional[str] = None):
        """
        Display and optionally save a 16-bit image with specified min/max values for visualization.
        Uses a local figure to avoid global state issues in multithreaded environments.
        """
        max_val = np.max(image)
        # Create a new figure and axes locally
        fig, ax = plt.subplots()
        im = ax.imshow(image, cmap='plasma', vmin=0, vmax=max_val)
        fig.colorbar(im, ax=ax)  # Adds a color scale bar to the current axes
        ax.axis('off')  # Hide axes for a cleaner look

        # Add a label if provided
        if label:
            ax.set_title(label, fontsize=12, pad=10)

        # Ensure the directory exists and save the figure
        save_path = Path(save_dir) / save_name
        save_path.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(save_path, bbox_inches='tight', pad_inches=0)
        plt.close(fig)  # Close the figure to free up memory
        if self.flag_logging:
            logging.info(f"Image saved at {save_path}")

    def create_image_array(self, binned_data: dict[dict], ref_coords: Optional[tuple] = None,
                           plot_scale: Optional[float] = None, save_path: Optional[Path] = None):
        """
        Arrange the averaged images into a sensibly sized grid and display them with scan parameter labels.
        For visualization purposes, images will be normalized to 8-bit.

        Args:
            binned_data (dict[dict]): List of averaged images. TODO for faster speed consider making a numpy array
            ref_coords (tuple): The x and y data to be plotted as a reference, as element 0 and 1, respectively
            plot_scale (float): A float value for the maximum color
        """
        if len(binned_data) == 0:
            if self.flag_logging:
                logging.warning("No averaged images to arrange into an array.")
            return

        # Calculate grid size for arranging images in a square-like layout
        num_images = len(binned_data)
        grid_cols = int(np.ceil(np.sqrt(num_images)))
        grid_rows = int(np.ceil(num_images / grid_cols))

        # get global color scale
        all_pixels = np.concatenate([binned_data[bnum]['image'].ravel()
                                     for bnum in list(binned_data.keys())
                                     if binned_data[bnum]['image'] is not None])
        low, high = 0, all_pixels.max()
        if plot_scale is not None:
            high = plot_scale

        # Create a figure with the appropriate number of subplots
        fig, axs = plt.subplots(grid_rows, grid_cols, figsize=(grid_cols * 3, grid_rows * 3))

        # Flatten axes array for easy indexing (if there's only one row/col, axs won't be a 2D array)
        axs = axs.flatten()

        bin_ind = None
        for bin_num, bin_item in binned_data.items():
            bin_ind = bin_num - 1
            img = bin_item['image']
            param_value = bin_item['value']

            # display with adjusted scale
            axs[bin_ind].imshow(img, cmap='plasma', vmin=low, vmax=high)
            axs[bin_ind].set_title(f'{self.scan_parameter}: {param_value:.2f}',
                                   fontsize=10)  # Use scan parameter for label
            axs[bin_ind].axis('off')  # Turn off axes for cleaner display

            if ref_coords is not None:
                axs[bin_ind].plot(ref_coords[0], ref_coords[1], color='g', marker='o')

        for j in range(bin_ind + 1, len(axs)):
            axs[j].axis('off')

        plt.tight_layout()

        # Save the final image grid for visualization
        if save_path:
            filename = save_path.name
        else:
            filename = f'{self.device_name}_averaged_image_grid.png'
            save_path = Path(self.path_dict['save']) / filename
        plt.savefig(save_path, bbox_inches='tight')
        if self.flag_logging:
            logging.info(f"Saved final image grid as {filename}.")
        self.close_or_show_plot()

    def get_image_analysis_result(self, shot_num: int, file_path: Optional[Path]) -> Optional[np.ndarray]:
        if file_path is None:
            logging.warning(f"No file path provided for shot {shot_num}.")
            return None

        try:
            # Use the injected image analyzer
            logging.info(f'attempting to process {file_path}')
            results_dict = self.image_analyzer.analyze_image_file(image_filepath=file_path)
            # Expecting results_dict to have keys: 'processed_image_uint16' and 'analyzer_return_dictionary'
            image = results_dict.get("processed_image_uint16")
            if image is not None:
                image = image.astype(np.uint16)
                self.data['shot_num'].append(shot_num)
                self.data['images'].append(image)

                # Update auxiliary data if analysis results are provided
                analysis_dict = results_dict.get("analyzer_return_dictionary", {})
                if analysis_dict:
                    for key, value in analysis_dict.items():
                        if not isinstance(value, numbers.Number):
                            logging.warning(
                                f"[{self.__class__.__name__} using {self.image_analyzer.__class__.__name__}] "
                                f"analysis result for shot {shot_num} key '{key}' is not a number (got {type(value).__name__}). Skipping."
                            )
                        else:
                            self.auxiliary_data.loc[self.auxiliary_data['Shotnumber'] == shot_num, key] = value

            return image

        except FileNotFoundError:
            logging.info(f"Image file not found for shot {shot_num}: {file_path}")
            return None
        except Exception as e:
            logging.error(f"Error processing shot {shot_num}: {e}")
            return None

    @staticmethod
    def average_images(images: list[np.ndarray]) -> Optional[np.ndarray]:
        """
        Average a list of images.

        Args:
            images (list of np.ndarray): List of images to average.

        Returns:
            np.ndarray: The averaged image.
        """
        if len(images) == 0:
            return None

        return np.mean(images, axis=0).astype(np.uint16)  # Keep 16-bit format for the averaged image

    @staticmethod
    def create_gif(image_arrays: List[np.ndarray], output_file: str,
                   titles: Optional[List[str]] = None, duration: float = 100, dpi: int = 72):
        """
        Create a GIF from a list of images with titles, scaled to a fixed width,
        and using the 'plasma' colormap.

        Args:
            image_arrays (List[np.ndarray]): List of images to include in the GIF.
            output_file (str): Path to save the resulting GIF.
            titles (Optional[List[str]]): List of titles for each image.
            duration (float): Duration for each frame in the GIF in milliseconds.
            dpi (int): The DPI for the images (default is 72 DPI).
        """
        # Desired width in pixels based on DPI
        target_width_inches = 5  # Width in inches
        target_width_pixels = int(target_width_inches * dpi)

        # Create default titles if not provided
        if titles is None:
            titles = [f"Shot {num + 1}" for num in range(len(image_arrays))]


        # Initialize the colormap and normalization
        cmap = plt.get_cmap('plasma')
        norm = Normalize(vmin=np.min(image_arrays), vmax=np.mean([img.max() for img in image_arrays]))

        # Font parameters for adding titles
        font = cv2.FONT_HERSHEY_TRIPLEX
        font_scale = 0.5
        thickness = 1
        color = (255, 255, 255)

        images = []
        for img, title in zip(image_arrays, titles):
            # Normalize the image and apply the colormap
            normalized_img = norm(img)
            colored_img = (cmap(normalized_img)[:, :, :3] * 255).astype(np.uint8)

            # Resize the image while maintaining the aspect ratio
            height, width, _ = colored_img.shape
            scale_factor = target_width_pixels / width
            target_height_pixels = int(height * scale_factor)
            resized_image = cv2.resize(colored_img, (target_width_pixels, target_height_pixels), interpolation=cv2.INTER_AREA)

            # Add title text
            (text_width, text_height), _ = cv2.getTextSize(title, font, font_scale, thickness)
            title_position = (max((target_width_pixels - text_width) // 2, 0), max(25, text_height + 10))

            # Add space for the title
            title_bar_height = 30
            title_image = np.zeros((title_bar_height + resized_image.shape[0], target_width_pixels, 3), dtype=np.uint8)
            title_image[title_bar_height:, :, :] = resized_image

            cv2.putText(title_image, title, title_position, font, font_scale, color, thickness)

            images.append(title_image)

        # Create GIF
        io.mimsave(output_file, images, duration=duration, loop=0)

if __name__ == "__main__":
    from scan_analysis.base import AnalyzerInfo as Info
    from scan_analysis.execute_scan_analysis import analyze_scan
    from geecs_python_api.controls.api_defs import ScanTag
    from image_analysis.offline_analyzers.Undulator.ACaveMagCam3 import ACaveMagCam3ImageAnalyzer

    perform_analysis = True
    analyzer_info = Info(analyzer_class=Array2DScanAnalysis,
                         requirements={'UC_ACaveMagCam3'},
                         device_name='UC_ACaveMagCam3',
                         image_analyzer_class=ACaveMagCam3ImageAnalyzer)

    # no scan example
    test_tag = ScanTag(year=2025, month=3, day=6, number=39, experiment='Undulator')

    # emq2 scan example
    test_tag = ScanTag(year=2025, month=3, day=7, number=22, experiment='Undulator')

    test_analyzer = analyzer_info
    import time
    t0 = time.monotonic()
    analyze_scan(test_tag, [analyzer_info], debug_mode=not perform_analysis)
    t1 = time.monotonic()
    print(t1-t0)
