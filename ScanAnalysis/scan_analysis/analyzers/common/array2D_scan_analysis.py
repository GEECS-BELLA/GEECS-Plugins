"""
Array2DScanAnalyzer

General analyzer for 2D array type data.
Child to ScanAnalyzer (./scan_analysis/base.py)

This provides a general framework for using an ImageAnalyzer to do analysis and post processing
over a scan.

It will generate some default type of visual representation of the data. For a 'noscan' it will
create an average image and a .gif. For a parameter scan, it will create an image array with each
individual image averaged over the bin # (e.g. scan parameter).

It can make use of multi threaded or multi processing of the images. The method applied is determined
by the attribute self.run_analyze_image_asynchronously in the ImageAnalyzer class. A setting of False
will use parallel processes and True will run multi threaded.

The method used for executing the ImageAnalysis.analyze_image method is _process_all_shots_parallel.
It is expected that the ImageAnalysis.analyze_image method will return a dict generated by the 'build_return_dict'
method of the base ImageAnalysis. This dict looks like this:
        return_dictionary = {
            "analyzer_input_parameters": input_parameters,
            "analyzer_return_dictionary": return_scalars,
            "processed_image": image of the type in the analyzer's return,
            "analyzer_return_lineouts": return_lineouts,
        }
The 'processed_image' item is the on that will be used to generate the visualized data.
The "analyzer_return_dictionary" contains a dict[str, float] that is used to write scalar
data to the 'sxxx.txt' file.
There is a rudimentary way to handle other types of return data using "analyzer_return_lineouts".
For example, a camera may be used as a spectrometer which is intended to generate a 1D array of
wavelength (energy) vs counts. Running this to through the default post_processing methods in
Array2DScanAnalyzer is not appropriate. By default, Array2DScanAnalyzer will ignore the lineout
data. But, child classes can be created to make use of the info by overwriting just a couple of
methods. An example is the HIMG_with_average_saving file. Creating a child class does still make
use of the other key parts of this framework (e.g. parallel processing, appending scalar data)

"""
# %% Imports
from __future__ import annotations

# --- Standard Library ---
import logging
import re
import traceback
import pickle
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import TYPE_CHECKING, Union, Optional, TypedDict, Any, Tuple

# --- Third-Party Libraries ---
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import imageio as io
import h5py
from collections import defaultdict

# --- Local / Project Imports ---
from scan_analysis.base import ScanAnalyzer
from image_analysis.base import ImageAnalyzer
from image_analysis.tools.rendering import base_render_image

from dataclasses import asdict

# --- Type-Checking Imports ---
if TYPE_CHECKING:
    from geecs_data_utils import ScanTag
    from numpy.typing import NDArray
    from image_analysis.types import AnalyzerResultDict

# --- Global Config ---
use_interactive = False
if not use_interactive:
    matplotlib.use("Agg")


PRINT_TRACEBACK = True

# --- TypedDict Definitions ---
class BinImageEntry(TypedDict):
    value: float
    result: Optional[AnalyzerResultDict]

# %% classes
class Array2DScanAnalyzer(ScanAnalyzer):

    def __init__(self, scan_tag: ScanTag,
                 device_name: str,
                 image_analyzer: Optional[ImageAnalyzer] = None,
                 file_tail: Optional[str] = '.png',
                 skip_plt_show: bool = True,
                 flag_logging: bool = True,
                 flag_save_images: bool = True):
        """
        Initialize the Array2DScanAnalyzer class.

        Args:
            scan_tag (ScanTag): tag used to identify the scan directory containing data.
            device_name (str): Name of the device to construct the subdirectory path.
            image_analyzer: ImageAnalyzer, instantiated ImageAnalyzer
            skip_plt_show (bool): Flag that sets if matplotlib is tried to use for plotting
            flag_logging (bool): Flag that sets if error and warning messages are displayed
            flag_save_images (bool): Flag that sets if images are saved to disk
        """
        if not device_name:
            raise ValueError("Array2DScanAnalyzer requires a device_name.")

        super().__init__(scan_tag, device_name=device_name,
                         skip_plt_show=skip_plt_show)

        self.image_analyzer = image_analyzer or ImageAnalyzer()

        self.max_workers = 16

        # define flags
        self.flag_logging = flag_logging
        self.flag_save_images = flag_save_images

        self.file_tail = file_tail

        try:
            pickle.dumps(self.image_analyzer)
        except (pickle.PicklingError, TypeError) as e:
            # Mark that we cannot send the ImageAnalyzer through multiprocessing,
            # so weâ€™ll fall back to threading instead.
            self.image_analyzer.run_analyze_image_asynchronously = True
            if self.flag_logging:
                logging.warning(
                    f"[Array2DScanAnalyzer] ImageAnalyzer instance is not pickleable "
                    f"(reason: {e}). Falling back to threaded analysis."
                )

        # organize various paths
        self.path_dict = {'data_img': Path(self.scan_directory) / f"{device_name}",
                          'save': (self.scan_directory.parents[1] / 'analysis' / self.scan_directory.name
                                   / f"{device_name}" / "Array2DScanAnalyzer")
                          }

        # Check if data directory exists and is not empty
        if not self.path_dict['data_img'].exists() or not any(self.path_dict['data_img'].iterdir()):
            if self.flag_logging:
                logging.warning(f"Data directory '{self.path_dict['data_img']}' does not exist or is empty. Skipping")

    def run_analysis(self):
        if self.path_dict['data_img'] is None or self.auxiliary_data is None:
            if self.flag_logging:
                logging.info("Skipping analysis due to missing data or auxiliary file.")
            return

        if self.flag_save_images and not self.path_dict['save'].exists():
            self.path_dict['save'].mkdir(parents=True)

        try:
            # Run the image analyzer on every shot in parallel.
            self._process_all_shots_parallel()


            # Depending on the scan type, perform additional processing.
            # self.results is a dict that only gets updated if the ImageAnalyzer
            # returns an image.
            if len(self.results) > 2:
                if self.noscan:
                    self._postprocess_noscan()
                else:
                    if use_interactive:
                        self._postprocess_scan_interactive()
                    else:
                        self._postprocess_scan_parallel()

            self.auxiliary_data.to_csv(self.auxiliary_file_path, sep='\t', index=False)
            return self.display_contents

        except Exception as e:
            if PRINT_TRACEBACK:
                print(traceback.format_exc())
            if self.flag_logging:
                logging.warning(f"Warning: Image analysis failed due to: {e}")
            return

    def _process_all_shots_parallel(self):
        self._load_all_images_parallel()
        self._run_batch_analysis()
        self._run_image_analysis_parallel()

    def _build_image_file_map(self) -> None:
        """
        Build a mapping from shot number to image file path using a flexible filename regex.
        Only includes files whose suffix + format matches `file_tail` exactly.

        """
        self._image_file_map = {}

        logging.info(f'self.file_tail: {self.file_tail}')
        image_filename_regex = re.compile(
            r"Scan(?P<scan_number>\d{3,})_"  # scan number
            r"(?P<device_subject>.*?)_"  # non-greedy subject
            r"(?P<shot_number>\d{3,})"  # shot number
            + re.escape(self.file_tail) + r"$"  # literal suffix+format
        )

        logging.info(f"mapping matched files")
        for file in self.path_dict['data_img'].iterdir():
            if not file.is_file():
                continue

            m = image_filename_regex.match(file.name)
            if m:
                shot_num = int(m.group('shot_number'))
                if shot_num in self.auxiliary_data['Shotnumber'].values:
                    self._image_file_map[shot_num] = file
                    logging.info(f"Mapped file for shot {shot_num}: {file}")
            else:
                logging.debug(f"Filename {file.name} does not match expected pattern.")

        expected_shots = set(self.auxiliary_data['Shotnumber'].values)
        found_shots = set(self._image_file_map.keys())
        for m in sorted(expected_shots - found_shots):
            logging.warning(f"No file found for shot {m}")

    def _load_all_images_parallel(self) -> None:
        """
        Load all images in parallel (threaded or multi-processed) and store them in self.raw_images.

        This method identifies the correct image file path for each shot number using the defined
        file pattern, and loads the corresponding images using the ImageAnalyzer's `load_image` method.
        Threading or multiprocessing is used based on the value of `self.image_analyzer.run_analyze_image_asynchronously`.

        Results are stored in:
            - self._image_file_map: {shot_number: Path}
            - self.raw_images: {shot_number: (np.ndarray, Path)}
        """
        self.raw_images = {}
        self._build_image_file_map()

        use_threads = self.image_analyzer.run_analyze_image_asynchronously
        Executor = ThreadPoolExecutor if use_threads else ProcessPoolExecutor

        with Executor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.image_analyzer.load_image, path): shot_num
                for shot_num, path in self._image_file_map.items()
            }

            for future in as_completed(futures):
                shot_num = futures[future]
                try:
                    image = future.result()
                    if image is not None:
                        file_path = self._image_file_map[shot_num]
                        self.raw_images[shot_num] = (image, file_path)
                except Exception as e:
                    logging.error(f"Error loading image for shot {shot_num}: {e}")

    def _run_batch_analysis(self) -> None:
        """
        Perform optional batch-level analysis across all loaded images.
        This uses the analyze_image_batch() method of ImageAnalysis, which expects a list of images.

        Raises:
            RuntimeError: If no images have been loaded yet or if output length is invalid.
        """
        if not hasattr(self, 'raw_images') or not self.raw_images:
            raise RuntimeError("No images loaded. Run _load_all_images_parallel first.")

        try:
            # Extract keys and separate image + path tuples
            shot_nums = list(self.raw_images.keys())
            image_path_tuples = list(self.raw_images.values())
            image_list = [img for img, _ in image_path_tuples]  # extract only images
            file_paths = [path for _, path in image_path_tuples]  # Reconstruct raw_images keeping original paths

            # Run batch analysis. This returns the processed images which will get handled
            # by analyze_image. It also returns additional otherwise stateful config type
            # results that should be used by analyze_image, but which are not explicitly
            # available to the multiprocessing based instances of the analyzer. Thus, they need
            # to be passed with the aux data
            processed_images, stateful_results = self.image_analyzer.analyze_image_batch(image_list)
            self.stateful_results = stateful_results

            if processed_images is None:
                logging.warning("analyze_image_batch() returned None. Skipping.")
                self.raw_images = {}
                return

            if len(processed_images) != len(shot_nums):
                raise ValueError(f"analyze_image_batch() returned {len(processed_images)} images, "
                                 f"but {len(shot_nums)} were expected.")

            self.raw_images = dict(zip(shot_nums, zip(processed_images, file_paths)))

        except Exception as e:
            logging.warning(f"Batch analysis skipped or failed: {e}")

    def _run_image_analysis_parallel(self) -> None:
        """
        Analyze each image in parallel (threaded or multi-processed).

        This method analyzes each image in `self.raw_images`, applying the `analyze_image()` method
        of the ImageAnalyzer instance.

        The analyzer is applied using either threads (True) or processes (False) based on the
        `run_analyze_image_asynchronously` flag. Results are stored in `self.results`.
        """
        logging.info('Starting the individual image analysis')
        self.results: dict[int, AnalyzerResultDict] = {}

        use_threads = self.image_analyzer.run_analyze_image_asynchronously
        Executor = ThreadPoolExecutor if use_threads else ProcessPoolExecutor
        logging.info(f'Using {"ThreadPoolExecutor" if use_threads else "ProcessPoolExecutor"}')

        with Executor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(
                    self.image_analyzer.analyze_image,
                    img,
                    {"file_path": path, **self.stateful_results}
                ): shot_num
                for shot_num, (img, path) in self.raw_images.items()
            }

            logging.info('Submitted image analysis tasks.')

            for future in as_completed(futures):
                shot_num = futures[future]
                try:
                    result: AnalyzerResultDict =  future.result()
                    processed_image = result.get("processed_image")
                    analysis_results = result.get("analyzer_return_dictionary", {})

                    if processed_image is not None:
                        self.results[shot_num] = result
                        logging.info(f"Shot {shot_num}: processed image stored.")
                        logging.info(f'analyzed shot {shot_num} and got {analysis_results}')

                    else:
                        logging.info(f"Shot {shot_num}: no image returned from analysis.")

                    for key, value in analysis_results.items():
                        if not isinstance(value, (int, float, np.number)):
                            logging.warning(
                                f"[{self.__class__.__name__} using {self.image_analyzer.__class__.__name__}] "
                                f"Analysis result for shot {shot_num} key '{key}' is not numeric (got {type(value).__name__}). Skipping."
                            )
                        else:
                            self.auxiliary_data.loc[self.auxiliary_data['Shotnumber'] == shot_num, key] = value

                except Exception as e:
                    logging.error(f"Analysis failed for shot {shot_num}: {e}")

    def _postprocess_noscan(self) -> None:
        """Perform post-processing for a no-scan: average images and create a GIF."""

        # Extract processed images and shot numbers from results dict
        images = [res["processed_image"] for res in self.results.values()]
        avg_image = self.average_images(images)

        if self.flag_save_images:
            # Save average image as HDF5
            self.save_image_as_h5(
                avg_image,
                save_dir=self.path_dict['save'],
                save_name=f'{self.device_name}_average_processed.h5'
            )

            # Save normalized PNG
            save_name = f'{self.device_name}_average_processed_visual.png'
            self.save_normalized_image(
                avg_image,
                save_dir=self.path_dict['save'],
                save_name=save_name,
                label=save_name
            )

            # Track for display
            display_content_path = Path(self.path_dict['save']) / save_name
            self.display_contents.append(str(display_content_path))

            # Create and store GIF
            gif_path = self.path_dict['save'] / 'noscan.gif'
            self.create_gif(data_dict=self.results, output_file=gif_path)
            self.display_contents.append(str(gif_path))

    def _save_bin_images(self, bin_key: int, processed_image: np.ndarray) -> None:
        """
        Helper method to save images for a single bin.
        This saves both the scaled and normalized images.
        """
        save_name_scaled = f"{self.device_name}_{bin_key}_processed.h5"
        save_name_normalized = f"{self.device_name}_{bin_key}_processed_visual.png"
        self.save_image_as_h5(processed_image,
                                     save_dir=self.path_dict["save"],
                                     save_name=save_name_scaled)
        self.save_normalized_image(processed_image,
                                   save_dir=self.path_dict["save"],
                                   save_name=save_name_normalized)
        if self.flag_logging:
            logging.info(f"Saved bin {bin_key} images: {save_name_scaled} and {save_name_normalized}")

    def _postprocess_scan_parallel(self) -> None:
        """
        Post-process a scanned variable by binning the images (from self.results) and then
        saving the resulting images in parallel.
        """
        # Use your existing parallel (or sequential) binning method to create binned_data.
        binned_data = self.bin_images_from_data(flag_save=False)

        # Save each bin's images concurrently using a thread pool.
        if self.flag_save_images:
            with ThreadPoolExecutor() as executor:
                futures = []
                for bin_key, bin_item in binned_data.items():
                    processed_image = bin_item['result']['processed_image']
                    futures.append(executor.submit(self._save_bin_images, bin_key, processed_image))
                for future in as_completed(futures):
                    # Optionally handle exceptions:
                    try:
                        future.result()
                    except Exception as e:
                        logging.error(f"Error saving images for a bin: {e}")

        # Create an image grid if more than one bin exists.
        if len(binned_data) > 1 and self.flag_save_images:
            plot_scale = (getattr(self, "camera_analysis_settings", {}) or {}).get("Plot Scale", None)
            save_path = Path(self.path_dict["save"]) / f'{self.device_name}_averaged_image_grid.png'
            self.create_image_array(binned_data, plot_scale=plot_scale, save_path=save_path)
            self.display_contents.append(str(save_path))
        self.binned_data = binned_data

    def _postprocess_scan_interactive(self) -> None:
        """Perform post-processing for a scan: bin images and create an image grid."""
        # Bin images from the already processed data.
        binned_data = self.bin_images_from_data(flag_save=False)

        # Process each bin sequentially.
        for bin_key, bin_item in binned_data.items():
            processed_image = bin_item['result'].get('processed_image')
            if self.flag_save_images and processed_image is not None:
                self._save_bin_images(bin_key, processed_image)
            elif processed_image is None:
                logging.warning(f"Bin {bin_key} has no processed image; skipping saving for this bin.")

        # If more than one bin exists, create an image grid.
        if len(binned_data) > 1 and self.flag_save_images:
            plot_scale = (getattr(self, "camera_analysis_settings", {}) or {}).get("Plot Scale", None)
            save_path = Path(self.path_dict["save"]) / f"{self.device_name}_averaged_image_grid.png"
            self.create_image_array(binned_data, plot_scale=plot_scale, save_path=save_path)
            self.display_contents.append(str(save_path))

        self.binned_data = binned_data

    def bin_images_from_data(self, flag_save: Optional[bool] = None) -> dict[int, BinImageEntry]:
        """
        Groups the processed images by bin value (as defined in self.auxiliary_data)
        and averages the images and scalar results for each bin.

        Args:
            flag_save (bool): Whether to save the binned images. Defaults to self.flag_save_images.

        Returns:
            dict: A dictionary mapping each bin number to a dictionary with:
                  - 'value': the scan parameter value for the bin
                  - 'image': the averaged image
                  - 'analysis_results': the averaged scalar results
        """
        if flag_save is None:
            flag_save = self.flag_save_images

        if "Bin #" not in self.auxiliary_data.columns:
            logging.warning("Missing 'Bin #' column in auxiliary data.")
            return {}

        unique_bins = [int(b) for b in np.unique(self.auxiliary_data["Bin #"].values)]
        if self.flag_logging:
            logging.info(f"Unique bins from auxiliary data: {unique_bins}")

        binned_data: dict[int, BinImageEntry] = {}

        for bin_val in unique_bins:
            # Get shot numbers in this bin
            bin_shots = self.auxiliary_data[self.auxiliary_data["Bin #"] == bin_val]["Shotnumber"].values

            # Filter shot numbers that have valid results
            valid_shots = [
                sn for sn in bin_shots
                if sn in self.results and self.results[sn].get("processed_image") is not None
            ]

            if not valid_shots:
                if self.flag_logging:
                    logging.warning(f"No images found for bin {bin_val}.")
                continue

            # Collect images and scalar results
            images = [self.results[sn]["processed_image"] for sn in valid_shots]
            analysis_results = [self.results[sn].get("analyzer_return_dictionary", {}) for sn in valid_shots]
            lineouts = [
                self.results[sn].get("analyzer_return_lineouts")
                for sn in valid_shots
                if isinstance(self.results[sn].get("analyzer_return_lineouts"), np.ndarray)
            ]
            # just extract the first entry in the input parameters, as it isn't expected to change
            input_params = self.results[valid_shots[0]].get("analyzer_input_parameters", {})

            avg_image = self.average_images(images)
            if avg_image is None:
                continue

            # calculate the average value for each key,item in analysis_results dict for the bin
            sums = defaultdict(list)
            for d in analysis_results:
                for k, v in d.items():
                    sums[k].append(v)
            avg_vals = {k: np.mean(v, axis=0) for k, v in sums.items()}

            if lineouts:
                lineouts_array = np.stack(lineouts)
                average_lineout = np.mean(lineouts_array, axis=0)
            else:
                average_lineout = None

            # Get representative scan parameter value
            column_full_name, _ = self.find_scan_param_column()
            param_value = self.auxiliary_data.loc[
                self.auxiliary_data["Bin #"] == bin_val, column_full_name
            ].mean()

            binned_data[bin_val] = {
                "value": float(param_value),
                "result": {
                    "processed_image": avg_image,
                    "analyzer_return_dictionary": avg_vals,
                    "analyzer_input_parameters": input_params,
                    "analyzer_return_lineouts": average_lineout
                }
            }

            if flag_save:
                save_name = f"{self.device_name}_{bin_val}.h5"
                self.save_image_as_h5(avg_image, save_dir=self.path_dict["save"], save_name=save_name)
                if self.flag_logging:
                    logging.info(f"Binned and averaged images for bin {bin_val} saved as {save_name}.")

        return binned_data

    def save_image_as_h5(self, image: NDArray, save_dir: Union[str, Path], save_name: str):
        """
        Saves the image as an HDF5 file using compression.

        Args:
            image (np.ndarray): Image to be saved.
            save_dir (str or Path): Directory where the image will be saved.
            save_name (str): The name of the saved HDF5 file (should end in .h5).
        """
        save_path = Path(save_dir) / save_name
        save_path.parent.mkdir(parents=True, exist_ok=True)

        with h5py.File(save_path, 'w') as f:
            f.create_dataset(
                'image',
                data=image,
                compression='gzip',  # Use GZIP compression
                compression_opts=4  # Compression level: 0 (none) to 9 (max)
            )

        if self.flag_logging:
            logging.info(f"HDF5 image saved with compression at {save_path}")

    def save_normalized_image(self, image: np.ndarray, save_dir: Union[str, Path], save_name: str,
                              label: Optional[str] = None):
        """
        Display and optionally save a 16-bit image with specified min/max values for visualization.
        Uses a local figure to avoid global state issues in multithreaded environments.
        """
        max_val = np.max(image)
        # Create a new figure and axes locally
        fig, ax = plt.subplots()
        im = ax.imshow(image, cmap='plasma', vmin=0, vmax=max_val)
        fig.colorbar(im, ax=ax)  # Adds a color scale bar to the current axes
        ax.axis('off')  # Hide axes for a cleaner look

        # Add a label if provided
        if label:
            ax.set_title(label, fontsize=12, pad=10)

        # Ensure the directory exists and save the figure
        save_path = Path(save_dir) / save_name
        save_path.parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(save_path, bbox_inches='tight', pad_inches=0)
        plt.close(fig)  # Close the figure to free up memory
        if self.flag_logging:
            logging.info(f"Image saved at {save_path}")

    def create_image_array(
            self,
            binned_data: dict[Union[int, float], BinImageEntry],
            plot_scale: Optional[float] = None,
            save_path: Optional[Path] = None,
            figsize: Tuple[float, float] = (4, 4),
            dpi: int = 150,
    ):
        """
        Arrange the averaged images into a grid and display them with scan parameter labels.

        Args:
            binned_data (dict): Mapping from bin number to BinImageEntry (which includes result dict).
            plot_scale (float, optional): Max color scale; defaults to global image max.
            save_path (Path, optional): Where to save the resulting figure.
            figsize (tuple): Size of each subplot in inches.
            dpi (int): DPI for rendering.
        """
        if not binned_data:
            if self.flag_logging:
                logging.warning("No averaged images to arrange into an array.")
            return

        num_images = len(binned_data)
        grid_cols = int(np.ceil(np.sqrt(num_images)))
        grid_rows = int(np.ceil(num_images / grid_cols))

        # Extract valid images from the result dicts
        images = [
            entry["result"]["processed_image"]
            for entry in binned_data.values()
            if entry["result"].get("processed_image") is not None
        ]
        vmin = 0
        vmax = plot_scale if plot_scale is not None else np.max([img.max() for img in images])

        render_fn = getattr(self.image_analyzer, "render_image", base_render_image)

        fig, axs = plt.subplots(
            grid_rows,
            grid_cols,
            figsize=(grid_cols * figsize[0], grid_rows * figsize[1]),
            dpi=dpi,
            constrained_layout=True,
        )
        axs = axs.flatten()
        fig.suptitle(f'Scan parameter: {self.scan_parameter}', fontsize=12)

        img_handle = None
        for idx, (bin_val, entry) in enumerate(binned_data.items()):
            if idx >= len(axs):
                break

            result = entry["result"]
            img = result.get("processed_image")
            if img is None:
                continue

            result = entry["result"]
            img = result.get("processed_image")
            analysis_results = result.get("analyzer_return_dictionary", {})
            input_params = result.get("analyzer_input_parameters", {})
            lineouts = result.get('analyzer_return_lineouts',[])
            param_val = entry.get("value", 0)

            render_fn(
                image=img,
                analysis_results_dict=analysis_results,
                input_params_dict=input_params,
                lineouts=lineouts,
                vmin=vmin,
                vmax=vmax,
                ax=axs[idx]
            )
            axs[idx].set_title(f'{param_val:.2f}', fontsize=10)

            if img_handle is None and axs[idx].images:
                img_handle = axs[idx].images[0]

        if img_handle:
            fig.colorbar(
                img_handle,
                ax=axs,
                orientation='horizontal',
                label='Intensity',
                shrink=0.8,
                pad=0.01,
                aspect=40,
            )

        if save_path is None:
            filename = f'{self.device_name}_averaged_image_grid.png'
            save_path = Path(self.path_dict['save']) / filename

        fig.savefig(save_path, bbox_inches='tight')
        plt.close(fig)

        if self.flag_logging:
            logging.info(f"Saved final image grid as {save_path.name}.")
        self.display_contents.append(str(save_path))

    def create_gif(
            self,
            data_dict: dict[Union[int, float], AnalyzerResultDict],
            output_file: Union[str, Path],
            sort_keys: bool = True,
            duration: float = 100,
            dpi: int = 150,
            figsize_inches: float = 4.0,
    ):
        """
        Create a GIF using a set of analysis results with custom rendering.

        Args:
            data_dict: Mapping from shot/bin number to AnalyzerResultDict.
            output_file: Path to save the resulting GIF.
            sort_keys: Whether to sort the keys (e.g., by shot number or scan value).
            duration: Duration of each frame in milliseconds.
            dpi: DPI for matplotlib rendering.
            figsize_inches: Width/height of each frame in inches.
        """
        output_file = Path(output_file)

        render_fn = getattr(self.image_analyzer, "render_image", base_render_image)

        frames = self.prepare_render_frames(data_dict, sort_keys=sort_keys)
        if not frames:
            if self.flag_logging:
                logging.warning("No valid frames to render into GIF.")
            return

        vmin = 0
        vmax = max(frame["image"].max() for frame in frames)

        gif_images = []
        for frame in frames:
            fig, ax = render_fn(
                image=frame["image"],
                analysis_results_dict=frame.get("analysis_results_dict", {}),
                input_params_dict=frame.get("input_params_dict", {}),
                lineouts=frame.get("return_lineouts", []),
                vmin=vmin,
                vmax=vmax,
                figsize=(figsize_inches, figsize_inches),
                dpi=dpi
            )
            ax.set_title(frame["title"], fontsize=10)

            # Convert rendered fig to RGB array
            fig.canvas.draw()
            rgb = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
            rgb = rgb.reshape(fig.canvas.get_width_height()[::-1] + (4,))[:, :, :3]  # drop alpha
            gif_images.append(rgb)

            plt.close(fig)

        # Save GIF
        io.mimsave(str(output_file), gif_images, duration=duration / 1000.0, loop=0)

        if self.flag_logging:
            logging.info(f"Saved GIF to {output_file.name}.")

        self.display_contents.append(str(output_file))

    @staticmethod
    def prepare_render_frames(
            data_dict: dict[Union[int, float], AnalyzerResultDict],
            sort_keys: bool = True
    ) -> list[dict]:
        """
        Prepare a list of frames from a dictionary of AnalyzerResultDicts,
        formatted for rendering via render_image.

        Args:
            data_dict: Dict mapping IDs (e.g. shot numbers or bin numbers) to AnalyzerResultDicts
            sort_keys: Whether to sort the keys before iterating

        Returns:
            List of dicts each containing image, title, and rendering metadata.
        """
        keys = sorted(data_dict) if sort_keys else data_dict.keys()
        frames = []

        for key in keys:
            result = data_dict[key]
            img = result.get("processed_image")
            if img is None:
                continue

            frames.append({
                "image": img,
                "title": f"{key}",
                "analysis_results_dict": result.get("analyzer_return_dictionary", {}),
                "input_params_dict": result.get("analyzer_input_parameters", {}),
                'return_lineouts': result.get("analyzer_return_lineouts", [])

            })

        return frames


    @staticmethod
    def average_images(images: list[np.ndarray]) -> Optional[np.ndarray]:
        """
        Average a list of images.

        Args:
            images (list of np.ndarray): List of images to average.

        Returns:
            np.ndarray: The averaged image.
        """
        if len(images) == 0:
            return None

        return np.mean(images, axis=0)

if __name__ == "__main__":

    from scan_analysis.base import ScanAnalyzerInfo as Info
    from scan_analysis.execute_scan_analysis import analyze_scan, instantiate_scan_analyzer
    from image_analysis.offline_analyzers.Undulator.BCaveMagSpecStitcher import BCaveMagSpecStitcherAnalyzer
    from image_analysis.offline_analyzers.Undulator.EBeamProfile import EBeamProfileAnalyzer

    from geecs_data_utils import ScanTag, ScanData

    dev_name = 'UC_ALineEBeam3'
    config_dict = {'camera_name': dev_name}
    analyzer_info = Info(scan_analyzer_class=Array2DScanAnalyzer,
                         requirements={dev_name},
                         device_name=dev_name,
                         scan_analyzer_kwargs={'image_analyzer':EBeamProfileAnalyzer(**config_dict)}
                         )

    import time
    t0 = time.monotonic()
    test_tag = ScanTag(year=2025, month=6, day=9, number=8, experiment='Undulator')
    scan_analyzer = instantiate_scan_analyzer(scan_analyzer_info=analyzer_info, scan_tag=test_tag)
    scan_analyzer.run_analysis()
    t1 = time.monotonic()
    logging.info(f'execution time: {t1-t0}')